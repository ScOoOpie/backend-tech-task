import time
from datetime import datetime, timezone
import logging
from contextlib import asynccontextmanager
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, Depends, Query
from sqlalchemy.orm import Session
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from .auth import APIKeyManager, get_current_user, require_admin_access, require_write_access, require_read_access
from .database import get_db, engine
from .models import Base, APIKey, User
from .schemas import APIKeyListResponse, EventBatch, AnalyticsResponse, GenerateAPIKeyRequest, GenerateAPIKeyResponse, UserCreateRequest, UserResponse, UsersListResponse
from .crud import ingest_events, get_user_stats, get_ingestion_metrics, clear_user_cache, get_cache_stats
from .analytics import *
from .middleware import RateLimiter
from .models import Event 
from .redis_client import redis_client
import asyncio
import uuid
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
import os
from .migrate import run_migrations
# –ú–æ–¥–µ–ª—ñ –ë–î
# Base.metadata.create_all(bind=engine)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π"""
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π handler —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –ø–æ —Ä–∞–∑–º–µ—Ä—É
    file_handler = RotatingFileHandler(
        filename=os.path.join(log_dir, 'app.log'),
        maxBytes=10 * 1024 * 1024,  # 10 MB
        backupCount=5,  # –•—Ä–∞–Ω–∏—Ç—å 5 backup —Ñ–∞–π–ª–æ–≤
        encoding='utf-8'
    )
    
    # Handler –¥–ª—è –æ—à–∏–±–æ–∫ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    error_handler = TimedRotatingFileHandler(
        filename=os.path.join(log_dir, 'errors.log'),
        when='W0',  # –ö–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é (–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫)
        backupCount=4,  # 4 –Ω–µ–¥–µ–ª–∏
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    
    # –§–æ—Ä–º–∞—Ç—Ç–µ—Ä
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    file_handler.setFormatter(formatter)
    error_handler.setFormatter(formatter)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            file_handler,
            error_handler,
            logging.StreamHandler()  # Console output
        ]
    )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–∏–Ω–≥
setup_logging()
logger = logging.getLogger(__name__)

# –ú–µ—Ç—Ä–∏–∫–∏
EVENTS_INGESTED_COUNTER = Counter('events_ingested_total', 'Total ingested events')
EVENTS_PUBLISHED_COUNTER = Counter('events_published_nats_total', 'Total events published to NATS')
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')
CACHE_HITS = Counter('cache_hits_total', 'Total cache hits', ['endpoint'])
CACHE_MISSES = Counter('cache_misses_total', 'Total cache misses', ['endpoint'])
REDIS_CONNECTION_GAUGE = Gauge('redis_connected', 'Redis connection status')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–ª—é—á–µ–π (–û–î–ò–ù –†–ê–ó)
api_key_manager = APIKeyManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("üöÄ Starting Event Analytics Service")
    
    # üîß –ü–†–ò–ú–ï–ù–Ø–ï–ú –ú–ò–ì–†–ê–¶–ò–ò –ü–†–ò –°–¢–ê–†–¢–ï
    logger.info("üì¶ Checking database migrations...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—Ç
    migration_success = await asyncio.get_event_loop().run_in_executor(
        None, 
        run_migrations
    )
    
    if not migration_success:
        logger.error("‚ùå Database migrations failed - application may not work correctly")
        # –í production –º–æ–∂–Ω–æ –≤—ã–π—Ç–∏, –≤ development –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        if os.getenv("ENVIRONMENT") == "production":
            raise RuntimeError("Database migrations failed")
    else:
        logger.info("‚úÖ Database migrations completed")
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º Redis
    await redis_client.connect()
    REDIS_CONNECTION_GAUGE.set(1 if redis_client.is_connected else 0)
    
    if redis_client.is_connected:
        logger.info("‚úÖ Redis connected successfully")
    else:
        logger.warning("‚ö†Ô∏è Redis connection failed - caching disabled")
    
    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ NATS (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
    try:
        from .nats_client import nats_client
        await nats_client.connect()
        app.state.nats_enabled = nats_client.is_connected
        if nats_client.is_connected:
            logger.info("‚úÖ NATS integration enabled")
        else:
            logger.info("‚ÑπÔ∏è  NATS integration disabled")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è NATS setup failed: {e}")
        app.state.nats_enabled = False
    
    # –î–æ–±–∞–≤–ª—è–µ–º Redis –≤ state –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    app.state.redis_enabled = redis_client.is_connected
    
    logger.info("‚úÖ All services initialized")
    yield
    
    # Shutdown
    logger.info("üõë Shutting down Event Analytics Service")
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º Redis
    await redis_client.close()
    REDIS_CONNECTION_GAUGE.set(0)
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º NATS
    try:
        from .nats_client import nats_client
        await nats_client.close()
    except:
        pass

app = FastAPI(
    title="Event Analytics Service",
    description="API –¥–ª—è –∑–±–æ—Ä—É —Ç–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –ø–æ–¥—ñ–π –∑ —Å–∏—Å—Ç–µ–º–æ—é –∞—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –∏ Redis –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º",
    version="1.1.0",  # –û–±–Ω–æ–≤–∏–ª–∏ –≤–µ—Ä—Å–∏—é –∏–∑-–∑–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è Redis
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Rate Limiter
rate_limiter = RateLimiter(capacity=1000, refill_rate=100)

async def get_rate_limiter():
    return rate_limiter

# ==================== REDIS UTILS ====================

async def get_cache_info() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ"""
    try:
        cache_stats = await get_cache_stats()
        return {
            "redis_enabled": getattr(app.state, 'redis_enabled', False),
            "cache_stats": cache_stats
        }
    except Exception as e:
        logger.error(f"Error getting cache info: {str(e)}")
        return {"redis_enabled": False, "error": str(e)}

# ==================== NATS UTILS ====================

async def publish_events_to_nats(events: List[Dict[str, Any]]) -> int:
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è —Å–æ–±—ã—Ç–∏–π –≤ NATS"""
    try:
        from .nats_client import nats_client
        
        if not nats_client.is_connected:
            return 0
            
        published_count = 0
        for event in events:
            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è NATS
            nats_message = {
                "event_id": str(event.event_id),
                "occurred_at": event.occurred_at.isoformat(),
                "user_id": event.user_id,
                "event_type": event.event_type,
                "properties": event.properties,
                "published_at": datetime.now(timezone.utc).isoformat(),
                "source": "analytics_api"
            }
            
            # –ü—É–±–ª–∏–∫—É–µ–º –≤ —Ä–∞–∑–Ω—ã–µ —Ç–æ–ø–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–æ–±—ã—Ç–∏—è
            subject = f"events.{event.event_type}"
            success = await nats_client.publish_event(subject, nats_message)
            
            if success:
                published_count += 1
                EVENTS_PUBLISHED_COUNTER.inc()
        
        logger.info(f"üì§ Published {published_count}/{len(events)} events to NATS")
        return published_count
        
    except Exception as e:
        logger.error(f"Error publishing events to NATS: {str(e)}")
        return 0

# ==================== EVENT ENDPOINTS ====================

@app.post("/events", status_code=201)
async def post_events(
    events: EventBatch,
    db: Session = Depends(get_db),
    limiter: RateLimiter = Depends(get_rate_limiter),
    user: dict = Depends(require_write_access)
):
    """
    –ù–∞–¥—Å–∏–ª–∞–Ω–Ω—è –ø–æ–¥—ñ–π –¥–ª—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **write**
    - –û–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤
    - –ó–±–µ—Ä—ñ–≥–∞—î –≤ –ë–î —Ç–∞ –ø—É–±–ª—ñ–∫—É—î –≤ NATS
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç Redis –∫—ç—à
    """
    start_time = time.time()
    
    # Rate limiting
    if not limiter.consume(1):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–±—ã—Ç–∏—è –≤ –ë–î
        ingested_count = await ingest_events(db, events.events)
        duration = time.time() - start_time
        REQUEST_DURATION.observe(duration)
        EVENTS_INGESTED_COUNTER.inc(ingested_count)
        
        # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏—è –≤ NATS (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç)
        if ingested_count > 0:
            asyncio.create_task(publish_events_to_nats(events.events))
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ
        cache_info = await get_cache_info()
        
        logger.info(f"üìù User {user['user_id']} ingested {ingested_count} events in {duration:.3f}s")
        
        return {
            "status": "success", 
            "ingested": ingested_count,
            "total_received": len(events.events),
            "processing_time": f"{duration:.3f}s",
            "nats_enabled": getattr(app.state, 'nats_enabled', False),
            "cache_enabled": cache_info["redis_enabled"],
            "cache_invalidated": True
        }
            
    except Exception as e:
        logger.error(f"Error ingesting events: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

# ==================== USER ENDPOINTS ====================

@app.post("/users", status_code=201)
async def create_user(
    user_id: str = Query(..., description="User ID"),
    name: str = Query(..., description="User name"),
    email: str = Query(None, description="User email"),
    db: Session = Depends(get_db),
    user: dict = Depends(require_admin_access)
):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        existing_user = db.query(User).filter(User.user_id == user_id).first()
        if existing_user:
            raise HTTPException(status_code=400, detail="User already exists")
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        new_user = User(
            user_id=user_id,
            name=name,
            email=email,
            is_active=True
        )
        
        db.add(new_user)
        db.commit()
        
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        if redis_client.is_connected:
            await redis_client.delete_pattern("user_stats:*")
            await redis_client.delete_pattern("ingestion_metrics:*")
        
        # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –≤ NATS
        try:
            from .nats_client import nats_client
            if nats_client.is_connected:
                user_event = {
                    "event_id": str(uuid.uuid4()),
                    "occurred_at": datetime.now(timezone.utc).isoformat(),
                    "user_id": user_id,
                    "event_type": "user_created",
                    "properties": {
                        "name": name,
                        "email": email,
                        "created_by": user['user_id']
                    },
                    "published_at": datetime.now(timezone.utc).isoformat()
                }
                await nats_client.publish_event("users.created", user_event)
        except Exception as e:
            logger.warning(f"Failed to publish user creation event: {e}")
        
        logger.info(f"Admin {user['user_id']} created user: {user_id}")
        
        return {
            "message": "‚úÖ User created successfully",
            "user_id": user_id,
            "name": name,
            "email": email,
            "cache_invalidated": redis_client.is_connected
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"Error creating user: {str(e)}")
        raise HTTPException(status_code=500, detail="Error creating user")

@app.get("/users")
async def list_users(
    active_only: bool = Query(True, description="Show only active users"),
    db: Session = Depends(get_db),
    user: dict = Depends(require_admin_access)
):
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    try:
        query = db.query(User)
        if active_only:
            query = query.filter(User.is_active == True)
            
        users = query.order_by(User.created_at.desc()).all()
        
        users_info = [
            {
                "user_id": u.user_id,
                "name": u.name,
                "email": u.email,
                "is_active": u.is_active,
                "created_at": u.created_at,
                "updated_at": u.updated_at
            }
            for u in users
        ]
        
        return {
            "total_users": len(users_info),
            "users": users_info,
            "cache_info": await get_cache_info()
        }
        
    except Exception as e:
        logger.error(f"Error listing users: {str(e)}")
        raise HTTPException(status_code=500, detail="Error listing users")

@app.get("/users/{user_id}")
async def get_user(
    user_id: str,
    db: Session = Depends(get_db),
    user: dict = Depends(require_read_access)
):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    try:
        db_user = db.query(User).filter(User.user_id == user_id).first()
        if not db_user:
            raise HTTPException(status_code=404, detail="User not found")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—É–∂–µ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
        user_stats = get_user_stats(db, user_id)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ
        cache_info = await get_cache_info()
        
        return {
            "user_id": db_user.user_id,
            "name": db_user.name,
            "email": db_user.email,
            "is_active": db_user.is_active,
            "created_at": db_user.created_at,
            "updated_at": db_user.updated_at,
            "stats": user_stats,
            "cache_info": {
                "redis_connected": cache_info["redis_enabled"],
                "from_cache": cache_info["redis_enabled"]  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω, —Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting user: {str(e)}")
        raise HTTPException(status_code=500, detail="Error getting user")

# ==================== ANALYTICS ENDPOINTS ====================

@app.get("/stats/dau")
async def get_dau(
    from_date: str,
    to_date: str,
    db: Session = Depends(get_db),
    user: dict = Depends(require_read_access)
) -> AnalyticsResponse:
    """
    –û—Ç—Ä–∏–º–∞–Ω–Ω—è DAU (Daily Active Users) —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **read**
    - –ü–∞—Ä–∞–º–µ—Ç—Ä–∏: from_date, to_date (—Ñ–æ—Ä–º–∞—Ç: YYYY-MM-DD)
    - –î–∞–Ω–Ω—ã–µ –∫—ç—à–∏—Ä—É—é—Ç—Å—è –≤ Redis
    """
    try:
        result = await get_dau_stats(db, from_date, to_date)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ –≤ –æ—Ç–≤–µ—Ç
        cache_info = await get_cache_info()
        result["cache_info"] = {
            "redis_connected": cache_info["redis_enabled"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return AnalyticsResponse(data=result)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error calculating DAU: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/stats/top-events")
async def get_top_events_stats(
    from_date: str,
    to_date: str,
    limit: int = 10,
    db: Session = Depends(get_db),
    user: dict = Depends(require_read_access)
) -> AnalyticsResponse:
    """
    –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–æ–ø—É –ø–æ–¥—ñ–π –∑–∞ —á–∞—Å—Ç–æ—Ç–æ—é
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **read**
    - –ü–∞—Ä–∞–º–µ—Ç—Ä–∏: from_date, to_date, limit
    - –î–∞–Ω–Ω—ã–µ –∫—ç—à–∏—Ä—É—é—Ç—Å—è –≤ Redis
    """
    try:
        result = await get_top_events(db, from_date, to_date, limit)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ
        cache_info = await get_cache_info()
        result["cache_info"] = {
            "redis_connected": cache_info["redis_enabled"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return AnalyticsResponse(data=result)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error calculating top events: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/stats/retention")
async def get_retention(
    start_date: str,
    windows: int = 7,
    db: Session = Depends(get_db),
    user: dict = Depends(require_read_access)
) -> AnalyticsResponse:
    """
    –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **read**
    - –ü–∞—Ä–∞–º–µ—Ç—Ä–∏: start_date (–¥–∞—Ç–∞ –∫–æ–≥–æ—Ä—Ç—ã), windows (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π)
    - –î–∞–Ω–Ω—ã–µ –∫—ç—à–∏—Ä—É—é—Ç—Å—è –≤ Redis
    """
    try:
        result = await get_retention_stats(db, start_date, windows)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ
        cache_info = await get_cache_info()
        result["cache_info"] = {
            "redis_connected": cache_info["redis_enabled"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return AnalyticsResponse(data=result)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Error calculating retention: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/cohorts/active")
async def get_active_cohorts(
    limit: int = Query(10, ge=1, le=50),
    db: Session = Depends(get_db),
    user: dict = Depends(require_read_access)
):
    """–°–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ–≥–æ—Ä—Ç —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        cohorts = await get_cohorts_list(db, limit)
        
        return {
            "cohorts": cohorts,
            "cache_info": await get_cache_info()
        }
    except Exception as e:
        logger.error(f"Error getting active cohorts: {str(e)}")
        raise HTTPException(status_code=500, detail="Error getting cohorts")

@app.get("/users/{user_id}/retention")
async def get_user_retention(
    user_id: str,
    db: Session = Depends(get_db),
    user: dict = Depends(require_read_access)
):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        stats = await get_user_retention_data(db, user_id)
        if "error" in stats:
            raise HTTPException(status_code=404, detail=stats["error"])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ
        cache_info = await get_cache_info()
        stats["cache_info"] = {
            "redis_connected": cache_info["redis_enabled"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return stats
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting user retention: {str(e)}")
        raise HTTPException(status_code=500, detail="Error getting user retention")

# ==================== CACHE MANAGEMENT ENDPOINTS ====================

@app.get("/cache/status")
async def get_cache_status(user: dict = Depends(require_admin_access)):
    """–°—Ç–∞—Ç—É—Å Redis –∫—ç—à–∞"""
    try:
        cache_info = await get_cache_info()
        return cache_info
    except Exception as e:
        logger.error(f"Error getting cache status: {str(e)}")
        raise HTTPException(status_code=500, detail="Error getting cache status")

@app.post("/cache/clear")
async def clear_cache(
    pattern: str = Query("cache:*", description="Pattern to clear"),
    user: dict = Depends(require_admin_access)
):
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
    try:
        if not redis_client.is_connected:
            return {
                "message": "Redis not connected",
                "pattern": pattern,
                "keys_deleted": 0
            }
        
        keys = await redis_client.client.keys(pattern)
        if keys:
            await redis_client.client.delete(*keys)
        
        logger.info(f"üóëÔ∏è Admin {user['user_id']} cleared cache pattern: {pattern}")
        
        return {
            "message": "‚úÖ Cache cleared successfully",
            "pattern": pattern,
            "keys_deleted": len(keys)
        }
    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        raise HTTPException(status_code=500, detail="Error clearing cache")

@app.post("/cache/users/{user_id}/clear")
async def clear_user_cache_endpoint(
    user_id: str,
    db: Session = Depends(get_db),
    user: dict = Depends(require_admin_access)
):
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        success = await clear_user_cache(user_id)
        
        return {
            "message": "‚úÖ User cache cleared successfully" if success else "‚ùå User cache clear failed",
            "user_id": user_id,
            "success": success
        }
    except Exception as e:
        logger.error(f"Error clearing user cache: {str(e)}")
        raise HTTPException(status_code=500, detail="Error clearing user cache")

@app.get("/cache/keys")
async def list_cache_keys(
    pattern: str = Query("cache:*", description="Key pattern"),
    limit: int = Query(100, ge=1, le=1000, description="Maximum keys to return"),
    user: dict = Depends(require_admin_access)
):
    """–°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π –≤ –∫—ç—à–µ"""
    try:
        if not redis_client.is_connected:
            return {
                "pattern": pattern,
                "keys_count": 0,
                "keys": [],
                "message": "Redis not connected"
            }
        
        keys = await redis_client.client.keys(pattern)
        
        return {
            "pattern": pattern,
            "keys_count": len(keys),
            "keys": keys[:limit]
        }
    except Exception as e:
        logger.error(f"Error listing cache keys: {str(e)}")
        raise HTTPException(status_code=500, detail="Error listing cache keys")

# ==================== SYSTEM ENDPOINTS ====================

@app.get("/system/metrics")
async def get_system_metrics(user: dict = Depends(require_admin_access)):
    """–°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∫–ª—é—á–∞—è Redis"""
    try:
        cache_info = await get_cache_info()
        ingestion_metrics = get_ingestion_metrics(db=next(get_db()))
        
        return {
            "cache": cache_info,
            "ingestion": ingestion_metrics,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting system metrics: {str(e)}")
        raise HTTPException(status_code=500, detail="Error getting system metrics")

@app.get("/metrics")
async def metrics(
    user: dict = Depends(require_admin_access)
):
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏"""
    return generate_latest()

@app.get("/health")
async def health_check():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è —Å–∏—Å—Ç–µ–º–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ Redis"""
    nats_enabled = getattr(app.state, 'nats_enabled', False)
    redis_enabled = getattr(app.state, 'redis_enabled', False)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Redis
    redis_healthy = False
    if redis_enabled and redis_client.is_connected:
        try:
            await redis_client.client.ping()
            redis_healthy = True
        except:
            redis_healthy = False
    
    return {
        "status": "healthy", 
        "nats_enabled": nats_enabled,
        "redis_enabled": redis_enabled,
        "redis_healthy": redis_healthy,
        "services": ["web", "db", "nats", "redis"],
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.get("/debug/pool-status")
async def pool_status():
    """–°—Ç–∞—Ç—É—Å –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ë–î"""
    pool = engine.pool
    status = {
        "pool_config": {
            "size": pool.size(),
            "max_overflow": pool._max_overflow,
            "timeout": pool.timeout,
            "recycle": pool._recycle
        },
        "current_usage": {
            "checkedout": pool.checkedout(),  # –ó–∞–Ω—è—Ç—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            "checkedin": pool.checkedin(),    # –°–≤–æ–±–æ–¥–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è  
            "overflow": pool.overflow(),      # –°–≤–µ—Ä—Ö –ª–∏–º–∏—Ç–∞
            "total": pool.checkedout() + pool.checkedin()
        },
        "status": "OK" if pool.checkedout() <= (pool.size() + pool._max_overflow) else "OVERLOAD",
        "redis_connected": getattr(app.state, 'redis_enabled', False)
    }
    return status

@app.get("/")
async def root():
    """–ö–æ—Ä–µ–Ω–µ–≤–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç"""
    cache_info = await get_cache_info()
    
    return {
        "message": "Event Analytics Service with Redis Caching", 
        "version": "1.1.0",
        "docs": "/docs",
        "health": "/health",
        "cache_enabled": cache_info["redis_enabled"],
        "features": [
            "Event ingestion",
            "User analytics", 
            "Cohort analysis",
            "Redis caching",
            "NATS integration",
            "API key authentication"
        ]
    }

# ==================== NATS ENDPOINTS ====================

@app.get("/nats/status")
async def get_nats_status():
    """–°—Ç–∞—Ç—É—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ NATS"""
    nats_enabled = getattr(app.state, 'nats_enabled', False)
    return {
        "nats_enabled": nats_enabled,
        "status": "connected" if nats_enabled else "disconnected"
    }

@app.post("/nats/publish-test")
async def publish_test_message(
    message: str = Query("Test message from API"),
    user: dict = Depends(require_admin_access)
):
    """–¢–µ—Å—Ç–æ–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è –≤ NATS"""
    try:
        from .nats_client import nats_client
        
        if not nats_client.is_connected:
            raise HTTPException(status_code=503, detail="NATS not connected")
        
        test_event = {
            "event_id": str(uuid.uuid4()),
            "occurred_at": datetime.now(timezone.utc).isoformat(),
            "user_id": user['user_id'],
            "event_type": "test_message",
            "message": message,
            "published_at": datetime.now(timezone.utc).isoformat()
        }
        
        success = await nats_client.publish_event("test.messages", test_event)
        
        if success:
            return {
                "message": "‚úÖ Test message published to NATS",
                "subject": "test.messages",
                "event_id": test_event["event_id"]
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to publish message")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error publishing test message: {str(e)}")
        raise HTTPException(status_code=500, detail="Error publishing test message")

# ==================== AUTH ENDPOINTS ====================

@app.post("/auth/generate-key", status_code=201, response_model=GenerateAPIKeyResponse)
async def generate_api_key(
    request: GenerateAPIKeyRequest,
    db: Session = Depends(get_db),
    user: dict = Depends(require_admin_access)
) -> GenerateAPIKeyResponse:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–æ–≥–æ API –∫–ª—é—á–∞
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **admin**
    - –ü–æ–≤–µ—Ä—Ç–∞—î –Ω–æ–≤–∏–π API –∫–ª—é—á (–ø–æ–∫–∞–∑—É—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑!)
    """
    try:
        new_key = api_key_manager.generate_key(
            db=db,
            user_id=request.user_id,
            name=request.name,
            permissions=request.permissions,
            expires_days=request.expires_days
        )
        
        logger.info(f"Admin {user['user_id']} generated key for {request.user_id}")
        
        return GenerateAPIKeyResponse(
            api_key=new_key,
            user_id=request.user_id,
            permissions=request.permissions,
            name=request.name,
            expires_days=request.expires_days
        )
    except Exception as e:
        logger.error(f"Error generating API key: {str(e)}")
        raise HTTPException(status_code=500, detail="Error generating API key")

@app.get("/auth/keys", response_model=APIKeyListResponse)
async def list_api_keys(
    user_id: Optional[str] = Query(None, description="Filter by user ID"),
    active_only: bool = Query(True, description="Show only active keys"),
    db: Session = Depends(get_db),
    user: dict = Depends(require_admin_access)
) -> APIKeyListResponse:
    """
    –°–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö API –∫–ª—é—á—ñ–≤
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **admin**
    - –ú–æ–∂–ª–∏–≤–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ user_id —Ç–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    """
    try:
        keys_info = api_key_manager.get_all_keys(db, user_id, active_only)
        
        return APIKeyListResponse(
            total_keys=len(keys_info),
            keys=keys_info
        )
    except Exception as e:
        logger.error(f"Error listing API keys: {str(e)}")
        raise HTTPException(status_code=500, detail="Error listing API keys")

@app.post("/auth/create-admin-key")
async def create_admin_key(
    db: Session = Depends(get_db)
):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–µ—Ä—à–æ–≥–æ –∞–¥–º—ñ–Ω –∫–ª—é—á–∞ (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)"""
    try:
        logger.info("Starting admin key creation process...")
        
        # üîß –ù–ê–î–ï–ñ–ù–´–ô –í–ê–†–ò–ê–ù–¢ - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤ Python
        all_active_keys = db.query(APIKey).filter(APIKey.is_active == True).all()
        existing_admin_keys = 0
        
        for key in all_active_keys:
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ JSON –ø–æ–ª—è
            if key.permissions and isinstance(key.permissions, list):
                if 'admin' in key.permissions:
                    existing_admin_keys += 1
                    logger.info(f"Found admin key: {key.name} for user {key.user_id}")
            else:
                logger.warning(f"Key {key.id} has invalid permissions format: {key.permissions}")
        
        logger.info(f"Total found {existing_admin_keys} existing admin keys")
        
        if existing_admin_keys > 0:
            logger.warning("Admin keys already exist, rejecting creation request")
            raise HTTPException(
                status_code=400, 
                detail="Admin keys already exist. Use regular key generation with admin access."
            )
        
        logger.info("No existing admin keys found, proceeding with creation...")
        
        # –°–æ–∑–¥–∞–µ–º –∞–¥–º–∏–Ω –∫–ª—é—á
        admin_key = api_key_manager.generate_key(
            db=db,
            user_id="system_admin",
            name="Initial System Admin Key",
            permissions=["read", "write", "admin"],
            expires_days=365
        )
        
        logger.info("‚úÖ Initial admin key created successfully")
        
        return {
            "message": "‚úÖ Initial admin key created successfully",
            "api_key": admin_key,
            "warning": "‚ö†Ô∏è Save this key securely! It will not be shown again.",
            "user_id": "system_admin",
            "permissions": ["read", "write", "admin"],
            "expires_in_days": 365
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error creating admin key: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error creating admin key")

@app.post("/auth/create-user-key")
async def create_user_key(
    user_id: str = Query(..., description="User ID for the new key"),
    key_name: str = Query(..., description="Name for the new key"),
    permissions: List[str] = Query(..., description="List of permissions"),
    expires_days: int = Query(30, description="Key expiration in days"),
    db: Session = Depends(get_db),
    admin_user: dict = Depends(require_admin_access)
):
    """
    –®–≤–∏–¥–∫–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫–æ–≥–æ –∫–ª—é—á–∞ —á–µ—Ä–µ–∑ query parameters
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **admin**
    - –ó—Ä—É—á–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–ª—é—á—ñ–≤
    """
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è permissions
        valid_permissions = ["read", "write", "admin"]
        for perm in permissions:
            if perm not in valid_permissions:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid permission: {perm}. Valid permissions are: {valid_permissions}"
                )
        
        user_key = api_key_manager.generate_key(
            db=db,
            user_id=user_id,
            name=key_name,
            permissions=permissions,
            expires_days=expires_days
        )

        logger.info(f"Admin {admin_user['user_id']} created key for user {user_id}")

        return {
            "message": "‚úÖ User API key created successfully",
            "api_key": user_key,
            "warning": "‚ö†Ô∏è Save this key securely! It will not be shown again.",
            "user_id": user_id,
            "key_name": key_name,
            "permissions": permissions,
            "expires_in_days": expires_days
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating user key: {str(e)}")
        raise HTTPException(status_code=500, detail="Error creating user key")

@app.delete("/auth/keys/{key_id}")
async def revoke_api_key(
    key_id: int,
    db: Session = Depends(get_db),
    user: dict = Depends(require_admin_access)
):
    """
    –í—ñ–¥–∫–ª–∏–∫–∞–Ω–Ω—è API –∫–ª—é—á–∞
    
    - –í–∏–º–∞–≥–∞—î –ø—Ä–∞–≤–∞ **admin**
    - –ö–ª—é—á –ø–æ–∑–Ω–∞—á–∞—î—Ç—å—Å—è —è–∫ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∏–π
    """
    try:
        success = api_key_manager.revoke_key(db, key_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="API key not found")
        
        logger.info(f"Admin {user['user_id']} revoked key ID {key_id}")
        
        return {
            "message": "‚úÖ API key revoked successfully",
            "key_id": key_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error revoking API key: {str(e)}")
        raise HTTPException(status_code=500, detail="Error revoking API key")

@app.get("/auth/my-keys", response_model=APIKeyListResponse)
async def get_my_api_keys(
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
) -> APIKeyListResponse:
    """
    –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–ª—é—á—ñ–≤ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    
    - –í–∏–º–∞–≥–∞—î –±—É–¥—å-—è–∫–∏–π –¥—ñ–π—Å–Ω–∏–π API –∫–ª—é—á
    - –ü–æ–≤–µ—Ä—Ç–∞—î —Ç—ñ–ª—å–∫–∏ –∫–ª—é—á—ñ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    """
    try:
        keys_info = api_key_manager.get_user_keys(db, current_user["user_id"])
        
        return APIKeyListResponse(
            total_keys=len(keys_info),
            keys=keys_info
        )
        
    except Exception as e:
        logger.error(f"Error getting user keys: {str(e)}")
        raise HTTPException(status_code=500, detail="Error getting user keys")

# ==================== SYSTEM ENDPOINTS ====================

@app.get("/metrics")
async def metrics(
    user: dict = Depends(require_admin_access)
):
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏"""
    return generate_latest()

@app.get("/health")
async def health_check():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è —Å–∏—Å—Ç–µ–º–∏"""
    nats_enabled = getattr(app.state, 'nats_enabled', False)
    return {
        "status": "healthy", 
        "nats_enabled": nats_enabled,
        "services": ["web", "db", "nats"],
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.get("/debug/pool-status")
async def pool_status():
    """–°—Ç–∞—Ç—É—Å –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ë–î"""
    pool = engine.pool
    status = {
        "pool_config": {
            "size": pool.size(),
            "max_overflow": pool._max_overflow,
            "timeout": pool.timeout,
            "recycle": pool._recycle
        },
        "current_usage": {
            "checkedout": pool.checkedout(),  # –ó–∞–Ω—è—Ç—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            "checkedin": pool.checkedin(),    # –°–≤–æ–±–æ–¥–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è  
            "overflow": pool.overflow(),      # –°–≤–µ—Ä—Ö –ª–∏–º–∏—Ç–∞
            "total": pool.checkedout() + pool.checkedin()
        },
        "status": "OK" if pool.checkedout() <= (pool.size() + pool._max_overflow) else "OVERLOAD"
    }
    return status

@app.get("/")
async def root():
    """–ö–æ—Ä–µ–Ω–µ–≤–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç"""
    return {
        "message": "Event Analytics Service", 
        "version": "1.0",
        "docs": "/docs",
        "health": "/health"
    }



