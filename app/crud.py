from sqlalchemy.orm import Session
from sqlalchemy import text, insert
from sqlalchemy.dialects.postgresql import insert as pg_insert
from typing import List, Set, Dict, Any
import logging
from sqlalchemy.exc import IntegrityError
from datetime import date, datetime
import uuid
from .cache import invalidate_cache, cached
from .redis_client import redis_client
from .models import Event, UserRetention, User, Base
from .schemas import Event as EventSchema

logger = logging.getLogger(__name__)

# ==================== –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ò–ù–ì–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ====================

@invalidate_cache(pattern="cohorts:list:*")
@invalidate_cache(pattern="retention:*")
@invalidate_cache(pattern="dau:*") 
@invalidate_cache(pattern="top_events:*")
@invalidate_cache(pattern="user_stats:*")
@invalidate_cache(pattern="ingestion_metrics:*")
async def ingest_events(db: Session, events: List[EventSchema]) -> int:
    """–ü–†–û–°–¢–ê–Ø –∏ –≠–§–§–ï–ö–¢–ò–í–ù–ê–Ø –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    if not events:
        return 0
    
    try:
        # 1. –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏–π
        events_data = []
        user_ids = set()
        
        for event in events:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event_id –≤ UUID –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            event_id = event.event_id
            if isinstance(event_id, str):
                try:
                    event_id = uuid.UUID(event_id)
                except (ValueError, AttributeError):
                    # –ï—Å–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π UUID, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
                    event_id = uuid.uuid4()
                    logger.warning(f"Invalid event_id format, generated new: {event_id}")
            
            events_data.append({
                'event_id': event_id,
                'occurred_at': event.occurred_at,
                'user_id': event.user_id,
                'event_type': event.event_type,
                'properties': event.properties,
                'event_date': event.occurred_at.date() if event.occurred_at else date.today()
            })
            user_ids.add(event.user_id)
        
        # 2. –ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        await _ensure_users_batch(db, user_ids)
        
        # 3. –í—Å—Ç–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏–π
        if events_data:
            db.execute(insert(Event).values(events_data))
        
        # 4. –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞
        await _update_retention_batch(db, events)
        
        # ‚úÖ –û–î–ò–ù –ö–û–ú–ú–ò–¢ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        db.commit()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∫—ç—à–µ–π
        await _invalidate_user_caches(user_ids)
        
        logger.info(f"‚úÖ Optimized batch: {len(events_data)} events")
        return len(events_data)
        
    except IntegrityError as e:
        # Fallback –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        db.rollback()
        logger.warning(f"‚ö†Ô∏è Batch insert failed due to duplicates, falling back: {str(e)}")
        return await _insert_events_individual_simple(db, events)
        
    except Exception as e:
        db.rollback()
        logger.error(f"‚ùå Batch insert failed: {str(e)}")
        return 0

async def _invalidate_user_caches(user_ids: Set[str]):
    """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    if not redis_client.is_connected:
        return
        
    try:
        for user_id in user_ids:
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            await redis_client.delete_pattern(f"user_stats:{user_id}:*")
            await redis_client.delete_pattern(f"user_retention:{user_id}:*")
        
        logger.info(f"üîÑ Invalidated caches for {len(user_ids)} users")
    except Exception as e:
        logger.error(f"Error invalidating user caches: {str(e)}")

# ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================

async def _ensure_users_batch(db: Session, user_ids: Set[str]):
    """–ü–∞–∫–µ—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ User"""
    if not user_ids:
        return
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º INSERT ... ON CONFLICT –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏ (–ª—É—á—à–∏–π –ø–æ–¥—Ö–æ–¥)
    users_data = []
    for user_id in user_ids:
        users_data.append({
            'user_id': user_id,
            'name': f"User_{user_id}",
            'email': f"{user_id}@example.com",
            'is_active': True,
            'created_at': datetime.utcnow(),
            'updated_at': datetime.utcnow()
        })
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
    stmt = pg_insert(User).values(users_data)
    stmt = stmt.on_conflict_do_update(
        index_elements=['user_id'],
        set_={
            'is_active': True,
            'updated_at': datetime.utcnow()
        }
    )
    db.execute(stmt)

async def _update_retention_batch(db: Session, events: List[EventSchema]):
    """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞ –¥–ª—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ UserRetention"""
    if not events:
        return
    
    user_ids = list({event.user_id for event in events})
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    first_events_query = text("""
        SELECT DISTINCT ON (e.user_id) 
            e.user_id, 
            e.occurred_at::date as first_date
        FROM events e
        WHERE e.user_id = ANY(:user_ids)
        ORDER BY e.user_id, e.occurred_at
    """)
    first_events = db.execute(first_events_query, {'user_ids': user_ids}).fetchall()
    first_event_map = {fe.user_id: fe.first_date for fe in first_events}
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞
    retention_data = []
    for event in events:
        user_id = event.user_id
        event_date = event.occurred_at.date() if event.occurred_at else date.today()
        cohort_date = first_event_map.get(user_id, event_date)
        retention_day = (event_date - cohort_date).days
        
        retention_data.append({
            'user_id': user_id,
            'cohort_date': cohort_date,
            'activity_date': event_date,
            'retention_day': retention_day
        })
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
    if retention_data:
        stmt = pg_insert(UserRetention).values(retention_data)
        stmt = stmt.on_conflict_do_nothing(
            index_elements=['user_id', 'cohort_date', 'activity_date']
        )
        db.execute(stmt)

async def _insert_events_individual_simple(db: Session, events: List[EventSchema]) -> int:
    """–ü—Ä–æ—Å—Ç–æ–π fallback –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –≤–∞—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    successful = 0
    user_ids = set()
    
    for event in events:
        try:
            event_date = event.occurred_at.date() if event.occurred_at else date.today()
            user_ids.add(event.user_id)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event_id –≤ UUID –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            event_id = event.event_id
            if isinstance(event_id, str):
                try:
                    event_id = uuid.UUID(event_id)
                except (ValueError, AttributeError):
                    event_id = uuid.uuid4()
                    logger.warning(f"Invalid event_id format in fallback, generated new: {event_id}")
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ON CONFLICT)
            user_stmt = pg_insert(User).values(
                user_id=event.user_id,
                name=f"User_{event.user_id}",
                email=f"{event.user_id}@example.com",
                is_active=True,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            user_stmt = user_stmt.on_conflict_do_update(
                index_elements=['user_id'],
                set_={
                    'is_active': True,
                    'updated_at': datetime.utcnow()
                }
            )
            db.execute(user_stmt)
            
            # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ
            db_event = Event(
                event_id=event_id,
                occurred_at=event.occurred_at,
                user_id=event.user_id,
                event_type=event.event_type,
                properties=event.properties,
                event_date=event_date
            )
            db.add(db_event)
            db.flush()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å IntegrityError
            successful += 1
            
        except IntegrityError:
            db.rollback()
            logger.debug(f"Skipped duplicate event: {event.event_id}")
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç
        except Exception as e:
            db.rollback()
            logger.error(f"Error inserting event individually: {str(e)}")
            continue
    
    # ‚úÖ –û–î–ò–ù –ö–û–ú–ú–ò–¢ –ø–æ—Å–ª–µ –≤—Å–µ—Ö —É—Å–ø–µ—à–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    if successful > 0:
        db.commit()
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à–∏ –¥–ª—è –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        await _invalidate_user_caches(user_ids)
    
    logger.info(f"üîÑ Fallback inserted: {successful} events")
    return successful

# ==================== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï CRUD –§–£–ù–ö–¶–ò–ò –° –ö–≠–®–ò–†–û–í–ê–ù–ò–ï–ú ====================

@cached(ttl=300, key_prefix="user_stats")  # 5 –º–∏–Ω—É—Ç TTL
def get_user_stats(db: Session, user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    from sqlalchemy import func
    
    logger.info(f"üìä Getting stats for user: {user_id}")
    
    # –ï—Å–ª–∏ Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ª–æ–≥–∏—Ä—É–µ–º –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
    if not redis_client.is_connected:
        logger.warning("Redis not connected, executing direct DB query for user stats")
    
    try:
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –ø–æ —Ç–∏–ø–∞–º
        events_by_type = db.query(
            Event.event_type,
            func.count(Event.id).label('count')
        ).filter(Event.user_id == user_id).group_by(Event.event_type).all()
        
        # –ü–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–±—ã—Ç–∏–µ
        first_event = db.query(Event).filter(
            Event.user_id == user_id
        ).order_by(Event.occurred_at.asc()).first()
        
        last_event = db.query(Event).filter(
            Event.user_id == user_id
        ).order_by(Event.occurred_at.desc()).first()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ—Ç–µ–Ω—à–µ–Ω–∞
        retention_stats = db.query(
            func.min(UserRetention.cohort_date).label('first_cohort'),
            func.max(UserRetention.activity_date).label('last_activity'),
            func.count(UserRetention.id).label('active_days')
        ).filter(UserRetention.user_id == user_id).first()
        
        result = {
            'events_by_type': {e.event_type: e.count for e in events_by_type},
            'total_events': sum(e.count for e in events_by_type),
            'first_event_at': first_event.occurred_at.isoformat() if first_event else None,
            'last_event_at': last_event.occurred_at.isoformat() if last_event else None,
            'retention_stats': {
                'first_cohort': retention_stats.first_cohort.isoformat() if retention_stats and retention_stats.first_cohort else None,
                'last_activity': retention_stats.last_activity.isoformat() if retention_stats and retention_stats.last_activity else None,
                'active_days_count': retention_stats.active_days if retention_stats else 0
            }
        }
        
        logger.info(f"‚úÖ User stats retrieved: {result['total_events']} events")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error getting user stats: {str(e)}")
        return {
            'events_by_type': {},
            'total_events': 0,
            'first_event_at': None,
            'last_event_at': None,
            'retention_stats': {
                'first_cohort': None,
                'last_activity': None,
                'active_days_count': 0
            }
        }

@cached(ttl=600, key_prefix="ingestion_metrics")  # 10 –º–∏–Ω—É—Ç TTL
def get_ingestion_metrics(db: Session) -> Dict[str, Any]:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∏–Ω–≥–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    from sqlalchemy import func
    
    logger.info("üìà Getting ingestion metrics")
    
    if not redis_client.is_connected:
        logger.warning("Redis not connected, executing direct DB query for metrics")
    
    try:
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ–±—ã—Ç–∏—è–º
        total_events = db.query(func.count(Event.id)).scalar()
        events_today = db.query(func.count(Event.id)).filter(
            Event.event_date == date.today()
        ).scalar()
        
        events_by_type = db.query(
            Event.event_type,
            func.count(Event.id).label('count')
        ).group_by(Event.event_type).order_by(func.count(Event.id).desc()).limit(10).all()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        total_users = db.query(func.count(User.id)).scalar()
        active_users_today = db.query(func.count(func.distinct(Event.user_id))).filter(
            Event.event_date == date.today()
        ).scalar()
        
        result = {
            'events': {
                'total': total_events or 0,
                'today': events_today or 0,
                'by_type': {e.event_type: e.count for e in events_by_type}
            },
            'users': {
                'total': total_users or 0,
                'active_today': active_users_today or 0
            },
            'retention': {
                'cohorts_count': db.query(func.count(func.distinct(UserRetention.cohort_date))).scalar() or 0,
                'retention_entries': db.query(func.count(UserRetention.id)).scalar() or 0
            },
            'cache_status': {
                'redis_connected': redis_client.is_connected,
                'timestamp': datetime.utcnow().isoformat()
            }
        }
        
        logger.info(f"‚úÖ Metrics retrieved: {result['events']['total']} total events")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error getting ingestion metrics: {str(e)}")
        return {
            'events': {'total': 0, 'today': 0, 'by_type': {}},
            'users': {'total': 0, 'active_today': 0},
            'retention': {'cohorts_count': 0, 'retention_entries': 0},
            'cache_status': {'redis_connected': False, 'error': str(e)}
        }

# ==================== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø –ö–≠–®–ï–ú ====================

async def clear_user_cache(user_id: str) -> bool:
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        if redis_client.is_connected:
            await redis_client.delete_pattern(f"user_stats:{user_id}:*")
            await redis_client.delete_pattern(f"user_retention:{user_id}:*")
            logger.info(f"‚úÖ Cleared cache for user: {user_id}")
            return True
        return False
    except Exception as e:
        logger.error(f"Error clearing user cache: {str(e)}")
        return False

async def get_cache_stats() -> Dict[str, Any]:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
    try:
        if not redis_client.is_connected:
            return {"redis_connected": False}
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª—é—á–∞—Ö
        cache_keys = await redis_client.client.keys("cache:*")
        user_stats_keys = await redis_client.client.keys("user_stats:*")
        metrics_keys = await redis_client.client.keys("ingestion_metrics:*")
        
        return {
            "redis_connected": True,
            "total_cache_keys": len(cache_keys),
            "user_stats_keys": len(user_stats_keys),
            "metrics_keys": len(metrics_keys),
            "memory_info": await redis_client.client.info('memory')
        }
    except Exception as e:
        logger.error(f"Error getting cache stats: {str(e)}")
        return {"redis_connected": False, "error": str(e)}